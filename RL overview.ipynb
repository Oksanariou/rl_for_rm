{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Markov Decision Process \nThe state transition and reward models $R$ and $T$ are known.\n\n### Value Iteration\nThe value function $U(s)$ represents the long-term reward that the agent is going to get if he starts in $s$ and follows the optimal policy.  \n\nThe value iteration approach keeps improving the value function at each iteration until it converges.\nAt each iteration and for each state s we update its estimated utility:  \n  $$U_{t+1}(s) \u003d \\max_{a}\\sum_{s\u0027}T(s, a, s\u0027)(R(s\u0027) + \\gamma U_t(s\u0027)) $$  \n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": "def value_iteration(env, gamma, max_iter, epsilon):\n    U \u003d np.zeros(env.nS)\n    for i in range(max_iter):\n        prev_U \u003d np.copy(U)\n        for s in range(env.nS):\n            list_sum \u003d np.zeros(env.nA)\n            for a in range(env.nA):\n                for p, s_prime, r, _ in env.P[s][a]:\n                    list_sum[a] +\u003d p*(r + gamma*prev_U[s_prime])\n            U[s] \u003d max(list_sum)\n        if (np.sum(np.fabs(prev_U - U)) \u003c\u003d epsilon):\n            break\n    return U     ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Once we have computed the true utility of each state $U(s)$ we can figure out the optimal policy $\\pi(s) \u003d \\underset{a}{\\operatorname{argmax}}\\sum_{s\u0027}T(s, a, s\u0027)U(s\u0027)$\n### Policy Iteration\nIf we compute the true utility of each state $U(s)$ we can figure out the optimal policy but we have much more information than what we need to figure out the optimal policy.  \nThe policy iteration approach re-defines the policy at each step and computes the value function associated to the current policy until the policy converges to the optimal policy.\nIt needs less iterations than VI to converge however each iteration is more computationally expensive.  \nGiven a policy $\\pi_t$ we compute the utility of each state:  \n  $$U_t(s) \u003d \\sum_{s\u0027}T(s, \\pi_t(s), s\u0027)(R(s\u0027) + \\gamma U_t(s\u0027)) $$",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": "def evaluate_policy(env, policy, gamma, epsilon):\n    U \u003d np.zeros(env.nS)\n    while True:\n        prev_U \u003d np.copy(U)\n        for s in range(env.nS):\n            a \u003d policy[s]\n            U[s] \u003d sum([p * (r + gamma * prev_U[s_]) for p, s_, r, _ in env.P[s][a]])\n            #for p, s_prime, r, _ in env.P[s][a]:\n                #U[s] +\u003d p*(r + gamma*prev_U[s_prime])\n        if (np.sum(np.fabs(prev_U - U)) \u003c\u003d epsilon):\n            break\n    return U",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "We then improve the policy:  \n$$ \\pi_{t+1}(s) \u003d \\underset{a}{\\operatorname{argmax}}\\sum_{s\u0027}T(s, a, s\u0027)U_t(s\u0027)$$",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": "def improve_policy(U):\n    policy \u003d np.zeros(env.nS)\n    for s in range(env.nS):\n        list_sum \u003d np.zeros(env.nA)\n        for a in range(env.nA):\n            for p, s_prime, r, _ in env.P[s][a]:\n                list_sum[a] +\u003d p*U[s_prime]\n        policy[s] \u003d np.argmax(list_sum)\n    return policy",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\nTo get the final Policy Iteration algorithm we combine the two previous steps:",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": "def policy_iteration(env, gamma, max_iter, epsilon):\n    policy \u003d np.random.choice(env.nA, env.nS)\n    for i in range(max_iter):\n        U \u003d evaluate_policy(env, policy, gamma, epsilon)\n        new_policy \u003d improve_policy(U)\n        if (np.all(policy \u003d\u003d new_policy)):\n            break\n        policy \u003d new_policy\n    return policy",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n## Reinforcement Learning\nThe state transition and reward models $T$ and $R$ are not known. The agent has access to the set of possible states and actions and has to learn through interactions with the environment.\n\n### Q-Learning\n\nThe Q-Learning algorithm does no longer have access to the models of the MDP that is to say the transition and reward functions.\nThe idea is now to evaluate the Bellman equation from data by using transitions (data : $ \u003cs, a, r, s\u0027\u003e$) to produce the solutions to the Q equations.\nAt each episode we are going to update the estimates of the Q function coming from the previous episode through a learning rate $\\alpha$. \n$$ Q(s, a) \u003d \\alpha(r + \\gamma \\max_{a\u0027}Q(s, a\u0027)) + (1 - \\alpha)Q(s, a)$$\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": "def q_learning(env, alpha, gamma, nb_episodes, nb_steps, epsilon, epsilon_min, epsilon_decay):\n    \n    # Initialize the Q-table with zeros\n    Q \u003d np.zeros([env.observation_space.n, env.action_space.n])\n    \n    for i in range(nb_episodes):\n        s \u003d env.reset() #Initial observation\n        for j in range(nb_steps):\n            # The action associated to s is the one that provides the best Q-value with a proba 1-epsilon and is random with a proba epsilon\n            if random.random() \u003c 1 - epsilon:\n                a \u003d np.argmax(Q[s,:]) \n            else : \n                a \u003d np.random.randint(env.action_space.n)\n            # We get our transition \u003cs, a, r, s\u0027\u003e\n            s_prime, r, d, _ \u003d env.step(a)\n            # We update the Q-tqble with using new knowledge\n            Q[s, a] \u003d alpha*(r + gamma*np.max(Q[s_prime,:])) + (1 - alpha)*Q[s, a]\n            s \u003d s_prime\n            if d \u003d\u003d True:\n                break\n            if (epsilon \u003e epsilon_min):\n                epsilon *\u003d epsilon_decay\n    \n    return Q\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Deep Q-Learning\n\nThe problem of the previous Q-Learning algorithm is that it will not be able to work in big state space environments.\nSo rather than using a Q-table which returns a Q-value for a given state and a given action we can implement a neural network $N$ which takes a state and returns the Q-values of all the possible actions that could be taken in that state: $N(s) \u003d \\{Q(s, a_1), Q(s, a_2), ..., Q(s, a_n)\\}$.   \n  Just as Q-Learning we start with an initial state $s$ and action $a$. We look at the next state $s\u0027$ and the associated reward $r$ that the agent receives when he takes this action $a$ in the state $s$.\n  The transition $\u003cs, a, r, s\u0027\u003e$ is stored in the memory of the agent. When we start to have enough transitions in the memory we sample a batch of them and for each transition $\u003cs, a_j, r, s\u0027\u003e$ we do the following:  \n   - Compute a target $t$, which represents the \"best\" action that can be done when the agent is in $s$, that is to say the action that maximizes the expected long-term reward.  \n   $t \u003d r +\\gamma \\max_a Q(s\u0027,a)$ with $\\{Q(s\u0027, a_i)\\}_{i \u003d 1, ..., n} \u003d N(s\u0027)$\n   - Compute the output predicted by the network for $s$: $N(s) \u003d \\{Q(s, a_i)\\}_{i \u003d 1, ..., n}$ \n   - Replace $Q(s, a_j)$ with $t$ to get $N\u0027(s)$\n   - Train the network using $s$ as the input and $N\u0027(s)$ as the output.  \n\nWe then use $s\u0027$ as the current state $s$ and reiterate.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "source": "class DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size \u003d state_size\n        self.action_size \u003d action_size\n        self.memory \u003d deque(maxlen\u003d2000)\n        self.gamma \u003d 0.95    # discount rate\n        self.epsilon \u003d 1.0  # exploration rate\n        self.epsilon_min \u003d 0.01\n        self.epsilon_decay \u003d 0.995\n        self.learning_rate \u003d 0.001\n        self.model \u003d self._build_model()\n\n    def _build_model(self):\n        # Neural Net for Deep-Q learning Model\n        model \u003d Sequential()\n        model.add(Dense(24, input_dim\u003dself.state_size, activation\u003d\u0027relu\u0027))\n        model.add(Dense(24, activation\u003d\u0027relu\u0027))\n        model.add(Dense(self.action_size, activation\u003d\u0027linear\u0027))\n        model.compile(loss\u003d\u0027mse\u0027, optimizer\u003dAdam(lr\u003dself.learning_rate))\n        return model\n\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state):\n        if np.random.rand() \u003c\u003d self.epsilon:\n            return random.randrange(self.action_size)\n        act_values \u003d self.model.predict(state)\n        return np.argmax(act_values[0])  # returns action\n\n    def replay(self, batch_size):\n        minibatch \u003d random.sample(self.memory, batch_size)\n        for state, action, reward, next_state, done in minibatch:\n            target \u003d reward\n            if not done:\n                target \u003d (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n            target_f \u003d self.model.predict(state)\n            target_f[0][action] \u003d target\n            self.model.fit(state, target_f, epochs\u003d1, verbose\u003d0)\n        if self.epsilon \u003e self.epsilon_min:\n            self.epsilon *\u003d self.epsilon_decay",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### Target network\nTwo networks are created. The first one is used to get the Q-values while the second one includes all updates in the training.\nThe parameters of the target network $\\theta^-$ are copied every $\\tau$ updates from the online network and kept fixed on all other steps.\nThanks to this target network the Q-value targets are fixed temporarily so that we do not have a moving target to chase. For each transition $\u003cs, a_j, r, s\u0027\u003e$ we do the following:\n- Use the target network to compute a target $t \u003d r +\\gamma \\max_a Q(s\u0027,a; \\theta^-)$\n- Use the online network to predict the output for $s$: $\\{Q(s, a_1;\\theta), ..., Q(s, a_n;\\theta)\\}$\n- Replace $Q(s, a_j;\\theta)$ with $t$ to get the corrected output for $s$\n- Train the online network using $s$ as the input and the corrected output as the output\n\n### Double Deep Q-Learning\nThe goal of Double Q-Learning is to reduce overestimations made on action values by DQL by decomposing the max operation present in the target $t$ into action selection and action evaluation.  \nIndeed the target computed using DQL can be written as $t_{DQL} \u003d r+\\gamma\\max_aQ(s\u0027,a;\\theta)$ where $\\theta$ represent the parameters of the network.  \nIn the DDQL algorithm two sets of weights $\\theta$ and $\\theta\u0027$ are used, one to determine the greedy policy and the other one to determine its value. \nThe Double Q-learning target can thus be written $t_{DDQL} \u003d r + \\gamma Q(s\u0027, \\underset{a}{\\operatorname{argmax}}Q(s\u0027,a;\\theta) ;\\theta\u0027)$. \nSo we use one network to see which action $a$ maximizes the Q value associated to $s\u0027$ and another network to evaluate the value of that action when it is associated to $s\u0027$. For instance we could used the online network as $\\theta$ and the target network $\\theta^-$ as $\\theta\u0027$.\n\n### Dueling Network Architectures\nInstead of using conventional architectures for DQL the idea is to use a dueling network representing two separate estimators. \nWe separate the representation of state values $U(s)$ and action advantages $A(s, a)$ in order to allow the architecture to learn which states are valuable without having to learn the effect of each action for each state.  \nThe two streams of the dueling Q-network are then combined via an aggregating layer to produce an estimate of $Q(s,a)$.\n### Categorical Deep Q Learning\n\n## Gym environments\n\n### Frozen Lake - 16 states\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "source": "import gym\nimport numpy as np\nimport random\n\nenv \u003d gym.make(\u0027FrozenLake-v0\u0027)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n#### Description\nStates: There are 16 different states that represent the different parts of the lake. An agent wants to go from a starting point (S) to a goal (G) situated on the other side of the lake.\nSome states are traversable (F) but others are holes (H) that lead the agent to fall into the water.\nThe surface can be represented using the following grid:  \n  SFFF  \n  FHFH  \n  FFFH  \n  HFFG\n\nActions: There are 4 different actions that the agent can do when he is in a state: [LEFT, DOWN, RIGHT, UP].\n\nTransition model: If the agent wishes to execute an action, this action is executed correctly with a probability of 0.8 and causes the agent to move at a right angle with a probability of 0.2.\nThe 0.2 is distributed uniformly over the two possible right angles.\n\nRewards: +1 if the agent reaches the goal, -1 if the agent falls down a hole and -0.04 if the agent is on a frozen surface.\n\n#### Value Iteration",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[ 0.10923213 -0.0224833   0.15217449 -0.0224833   0.18551446  0.\n  0.258482    0.          0.39985603  0.68188442  0.64537105  0.\n  0.          0.81919453  0.94288425  0.        ]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "env \u003d env.unwrapped\ngamma \u003d 0.95\nmax_iter \u003d 100000\nepsilon \u003d 1e-20\nprint(value_iteration(env, gamma, max_iter, epsilon))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "To visualize the policy produced by this utility:",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "DRDL\nDHDH\nRDDH\nHRDH\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "def visualize_policy(policy):\n    visu \u003d \u0027\u0027\n    for k in range(len(policy)):\n        if k \u003e 0 and k%4 \u003d\u003d 0:\n            visu +\u003d \u0027\\n\u0027\n        if k \u003d\u003d 5 or k \u003d\u003d 7 or k \u003d\u003d 11 or k \u003d\u003d 12 or k \u003d\u003d 15:\n            visu+\u003d\u0027H\u0027\n        elif int(policy[k]) \u003d\u003d 0:\n            visu +\u003d \u0027L\u0027\n        elif int(policy[k]) \u003d\u003d 1:\n            visu +\u003d \u0027D\u0027\n        elif int(policy[k]) \u003d\u003d 2:\n            visu +\u003d \u0027R\u0027\n        elif int(policy[k]) \u003d\u003d 3:\n            visu +\u003d \u0027U\u0027\n    print(visu)\n    \nU \u003d value_iteration(env, gamma, max_iter, epsilon)\npolicy \u003d improve_policy(U)\nvisualize_policy(policy)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### Policy Iteration",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "UDDD\nRHDH\nDRDH\nHRDH\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "policy \u003d policy_iteration(env, gamma, max_iter, epsilon)\nvisualize_policy(policy)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### Q-Learning",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-4-bd11a83ac345\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mepsilon_min\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mepsilon_decay\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;36m0.9999\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----\u003e 6\u001b[1;33m \u001b[0mq_table\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name \u0027q_learning\u0027 is not defined"
          ],
          "ename": "NameError",
          "evalue": "name \u0027q_learning\u0027 is not defined",
          "output_type": "error"
        }
      ],
      "source": "alpha, gamma \u003d 0.05, 0.99\nnb_episodes, nb_steps \u003d 100, 100\nepsilon \u003d 1\nepsilon_min \u003d 0.01\nepsilon_decay \u003d 0.9999\nq_table \u003d q_learning(env, alpha, gamma, nb_episodes, nb_steps, epsilon, epsilon_min, epsilon_decay)\nprint(q_table)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "If we look at the action which maximizes the Q-value for each state we can visualize the policy produced by the Q-table:  ",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "DLDL\nDHDH\nRDLH\nHRRH\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "def q_to_policy(Q):\n    policy \u003d []\n    for l in Q:\n        if l[0] \u003d\u003d l[1] \u003d\u003d l[2] \u003d\u003d l[3] \u003d\u003d 0.0:\n            policy.append(0)\n        else:\n            for k in range(0, len(l)):\n                if l[k] \u003d\u003d max(l):\n                    policy.append(k)\n                    break\n    return policy\n                    \npolicy \u003d q_to_policy(q_table)\nvisualize_policy(policy)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "##### Different exploration strategies\nTo compare the effect of different exploration strategies on this non-deterministic Q-Learning algorithm we decided to compute $100$ policies for each strategy and to look at the mean of their average total rewards.  \n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "source": "def run_episode(env, policy):\n    \"\"\" Runs an episode and returns the total reward \"\"\"\n    obs \u003d env.reset()\n    total_reward \u003d 0\n    while True:\n        obs, reward, done, _ \u003d env.step(int(policy[obs]))\n        total_reward +\u003d reward\n        if done:\n            break\n    return total_reward\n\ndef evaluate_policy(env, policy, n_eval):\n    \"\"\" Runs n episodes and returns the average of the n total rewards\"\"\"\n    scores \u003d [run_episode(env, policy) for _ in range(n_eval)]\n    return np.mean(scores)\n\ndef running_q_learning_n_times(n, n_eval, env, alpha, gamma, nb_episodes, nb_steps, epsilon, epsilon_min, epsilon_decay):\n    scores \u003d []\n    for k in range(n):\n        print(k)\n        q_table \u003d q_learning(env, alpha, gamma, nb_episodes, nb_steps, epsilon, epsilon_min, epsilon_decay)\n        policy \u003d q_to_policy(q_table)\n        score \u003d evaluate_policy(env, policy, n_eval)\n        scores.append(score)\n    return np.mean(scores)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "The following function is used to visualize the evolution of epsilon over time.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "source": "import matplotlib.pyplot as plt\ndef visualizing_epsilon_decay(nb_episodes, epsilon, epsilon_min, epsilon_decay):\n    X \u003d [k for k in range(nb_episodes)]\n    Y \u003d []\n    for k in range(nb_episodes):\n        if epsilon \u003e epsilon_min:\n            epsilon *\u003d epsilon_decay\n        Y.append(epsilon)\n    plt.plot(X, Y, \u0027b\u0027)\n    plt.title(\"Decaying epsilon over the number of episodes\")\n    plt.xlabel(\"Number of episodes\")\n    plt.ylabel(\"Epsilon\")\n    plt.grid()\n    return plt.show()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "###### Epsilon decaying slowly over the number of episodes",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [
        {
          "data": {
            "text/plain": "\u003cFigure size 432x288 with 1 Axes\u003e",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "text": [
            "RURR\nDHUH\nURLH\nHRRH\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "epsilon \u003d 1\nepsilon_min \u003d 0.01\nepsilon_decay \u003d 0.9999\nvisualizing_epsilon_decay(nb_episodes, epsilon, epsilon_min, epsilon_decay)\nq_table \u003d q_learning(env, alpha, gamma, nb_episodes, nb_steps, epsilon, epsilon_min, epsilon_decay)\npolicy \u003d q_to_policy(q_table)\nvisualize_policy(policy)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Average total reward: 0.25\n###### Epsilon decaying quickly over the number of episodes\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [
        {
          "data": {
            "text/plain": "\u003cFigure size 432x288 with 1 Axes\u003e",
            "image/png": "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\u003d\n"
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "text": [
            "RRDU\nDHDH\nRDLH\nHRRH\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "epsilon \u003d 1\nepsilon_min \u003d 0.01\nepsilon_decay \u003d 0.97\nvisualizing_epsilon_decay(nb_episodes, epsilon, epsilon_min, epsilon_decay)\nq_table \u003d q_learning(env, alpha, gamma, nb_episodes, nb_steps, epsilon, epsilon_min, epsilon_decay)\npolicy \u003d q_to_policy(q_table)\nvisualize_policy(policy)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Average total reward: 0.23\nOverall we get a slightly better average total reward when we decay epsilon slowly than when we do it quickly, which shows that for this environment it is important to keep exploring and not focus more on exploitation than exploration too quickly.\n#### Deep Q-Learning\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From c:\\users\\oriou\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n",
            "WARNING:tensorflow:From c:\\users\\oriou\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "from collections import deque\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\nstate_size \u003d 1\naction_size \u003d env.action_space.n\nagent \u003d DQNAgent(state_size, action_size)\ndone \u003d False\nbatch_size \u003d 32\n\nfor e in range(nb_episodes):\n    state \u003d env.reset()\n    state \u003d np.reshape(state, [1, state_size])\n    for time in range(nb_steps):\n        action \u003d agent.act(state)\n        next_state, reward, done, _ \u003d env.step(action)\n        next_state \u003d np.reshape(next_state, [1, state_size])\n        agent.remember(state, action, reward, next_state, done)\n        state \u003d next_state\n        if done:\n            break\n        if len(agent.memory) \u003e batch_size:\n            agent.replay(batch_size)\n                \ntrained_network \u003d agent.model",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "If we use the trained network to predict the Q-values of all the possible actions for each state we get the following Q-table:",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[[-0.36276409 -0.36358738 -0.34956151 -0.36803347]\n [-0.42438653 -0.56051075 -0.45745918 -0.42576566]\n [-0.38929811 -0.4887521  -0.41695005 -0.39924663]\n [-0.39063635 -0.50787544 -0.43388253 -0.42860413]\n [-0.32098693 -0.36689979 -0.37309313 -0.40078509]\n [-0.35560364 -0.44155252 -0.33484015 -0.41989574]\n [-0.43818772 -0.57127154 -0.23598194 -0.40575978]\n [-0.46052262 -0.59900045 -0.14967215 -0.38327429]\n [-0.39900315 -0.4420557  -0.10320634 -0.3309949 ]\n [-0.27884251 -0.10636949 -0.11077434 -0.23671144]\n [-0.15868193  0.22931662 -0.11834252 -0.14242798]\n [-0.04899368  0.48229748  0.0397675  -0.00902036]\n [ 0.02124009  0.60702127  0.3495158   0.09754792]\n [ 0.09147385  0.73174506  0.65926361  0.20411602]\n [ 0.16170761  0.85646874  0.96901166  0.31068417]\n [ 0.23194143  0.98119253  1.27875996  0.41725245]]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "def compute_q_table(env, network):\n    q_table \u003d np.zeros([env.observation_space.n, env.action_space.n])\n    for s in range(env.observation_space.n):\n        state \u003d np.zeros(1, int)\n        state[0] \u003d s\n        state \u003d np.reshape(state, [1, state_size])\n        q_table[s] \u003d list(network.predict(state)[0])\n    return q_table\n\nprint(compute_q_table(env, trained_network))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "The policy produced by this Q-table is the following one:",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "RLLL\nLHRH\nRDDH\nHDRH\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "q_table \u003d compute_q_table(env, trained_network)\npolicy \u003d q_to_policy(q_table)\nvisualize_policy(policy)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}