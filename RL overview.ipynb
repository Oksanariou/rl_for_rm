{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Markov Decision Process \nThe state transition and reward models $R$ and $T$ are known.\n\n### Value Iteration\nThe value function $U(s)$ represents the long-term reward that the agent is going to get if he starts in $s$ and follows the optimal policy.  \n\nThe value iteration approach keeps improving the value function at each iteration until it converges.\nAt each iteration and for each state s we update its estimated utility:  \n  $$U_{t+1}(s) \u003d \\max_{a}\\sum_{s\u0027}T(s, a, s\u0027)(R(s\u0027) + \\gamma U_t(s\u0027)) $$  \n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "source": "def value_iteration(env, gamma, max_iter, epsilon):\n    U \u003d np.zeros(env.nS)\n    for i in range(max_iter):\n        prev_U \u003d np.copy(U)\n        for s in range(env.nS):\n            list_sum \u003d np.zeros(env.nA)\n            for a in range(env.nA):\n                for p, s_prime, r, _ in env.P[s][a]:\n                    list_sum[a] +\u003d p*(r + gamma*prev_U[s_prime])\n            U[s] \u003d max(list_sum)\n        if (np.sum(np.fabs(prev_U - U)) \u003c\u003d epsilon):\n            break\n    return U     ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Once we have computed the true utility of each state $U(s)$ we can figure out the optimal policy $\\pi(s) \u003d \\underset{a}{\\operatorname{argmax}}\\sum_{s\u0027}T(s, a, s\u0027)U(s\u0027)$\n### Policy Iteration\nIf we compute the true utility of each state $U(s)$ we can figure out the optimal policy but we have much more information than what we need to figure out the optimal policy.  \nThe policy iteration approach re-defines the policy at each step and computes the value function associated to the current policy until the policy converges to the optimal policy.\nIt needs less iterations than VI to converge however each iteration is more computationally expensive.  \nGiven a policy $\\pi_t$ we compute the utility of each state:  \n  $$U_t(s) \u003d \\sum_{s\u0027}T(s, \\pi_t(s), s\u0027)(R(s\u0027) + \\gamma U_t(s\u0027)) $$",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "source": "def evaluate_policy(env, policy, gamma, epsilon):\n    U \u003d np.zeros(env.nS)\n    while True:\n        prev_U \u003d np.copy(U)\n        for s in range(env.nS):\n            a \u003d policy[s]\n            U[s] \u003d sum([p * (r + gamma * prev_U[s_]) for p, s_, r, _ in env.P[s][a]])\n            #for p, s_prime, r, _ in env.P[s][a]:\n                #U[s] +\u003d p*(r + gamma*prev_U[s_prime])\n        if (np.sum(np.fabs(prev_U - U)) \u003c\u003d epsilon):\n            break\n    return U",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "We then improve the policy:  \n$$ \\pi_{t+1}(s) \u003d \\underset{a}{\\operatorname{argmax}}\\sum_{s\u0027}T(s, a, s\u0027)U_t(s\u0027))$$",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": "def improve_policy(U, gamma):\n    policy \u003d np.zeros(env.nS)\n    for s in range(env.nS):\n        list_sum \u003d np.zeros(env.nA)\n        for a in range(env.nA):\n            for p, s_prime, r, _ in env.P[s][a]:\n                list_sum[a] +\u003d p*U[s_prime]\n        policy[s] \u003d np.argmax(list_sum)\n    return policy",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\nTo get the final Policy Iteration algorithm we combine the two previous steps:",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": "def policy_iteration(env, gamma, max_iter, epsilon):\n    policy \u003d np.random.choice(env.nA, env.nS)\n    for i in range(max_iter):\n        U \u003d evaluate_policy(env, policy, gamma, epsilon)\n        new_policy \u003d improve_policy(U, gamma)\n        if (np.all(policy \u003d\u003d new_policy)):\n            break\n        policy \u003d new_policy\n    return policy",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n## Reinforcement Learning\nThe state transition and reward models $T$ and $R$ are not known. The agent has access to the set of possible states and actions and has to learn through interactions with the environment.\n\n### Q-Learning\n\nThe Q-Learning algorithm does no longer have access to the models of the MDP that is to say the transition and reward functions.\nThe idea is now to evaluate the Bellman equation from data by using transitions (data : $ \u003cs, a, r, s\u0027\u003e$) to produce the solutions to the Q equations.\nAt each episode we are going to update the estimates of the Q function coming from the previous episode through a learning rate $\\alpha$. \n$$ Q(s, a) \u003d \\alpha(r + \\gamma \\max_{a\u0027}Q(s, a\u0027)) + (1 - \\alpha)Q(s, a)$$\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": "def q_learning(env, alpha, gamma, nb_episodes, nb_steps, epsilon):\n    \n    # Initialize the Q-table with zeros\n    Q \u003d np.zeros([env.observation_space.n, env.action_space.n])\n    \n    for i in range(nb_episodes):\n        s \u003d env.reset() #Initial observation\n        for j in range(nb_steps):\n            # The action associated to s is the one that provides the best Q-value with a proba 1-epsilon and is random with a proba epsilon\n            if random.random() \u003c 1 - epsilon:\n                a \u003d np.argmax(Q[s,:]) \n            else : \n                a \u003d np.random.randint(env.action_space.n)\n            # We get our transition \u003cs, a, r, s\u0027\u003e\n            s_prime, r, d, _ \u003d env.step(a)\n            # We update the Q-tqble with using new knowledge\n            Q[s, a] \u003d alpha*(r + gamma*np.max(Q[s_prime,:])) + (1 - alpha)*Q[s, a]\n            s \u003d s_prime\n            if d \u003d\u003d True:\n                break\n    \n    return Q\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Deep Q-Learning\n\nThe problem of the previous Q-Learning algorithm is that it will not be able to work in big state space environments.\nSo rather than using a Q-table which returns a Q-value for a given state and a given action we can implement a neural network $N$ which takes a state and returns the Q-values of all the possible actions that could be taken in that state: $N(s) \u003d \\{Q(s, a_1), Q(s, a_2), ..., Q(s, a_n)\\}$.   \n  Just as Q-Learning we start with an initial state $s$ and action $a$. We look at the next state $s\u0027$ and the associated reward $r$ that the agent receives when he takes this action $a$ in the state $s$.\n  The transition $\u003cs, a, r, s\u0027\u003e$ is stored in the memory of the agent. When we start to have enough transitions in the memory we sample a batch of them and for each transition $\u003cs, a_j, r, s\u0027\u003e$ we do the following:  \n   - Compute a target $t$, which represents the \"best\" action that can be done when the agent is in $s$, that is to say the action that maximizes the expected long-term reward.  \n   $t \u003d r +\\gamma \\max_a Q(s\u0027,a)$ with $\\{Q(s\u0027, a_i)\\}_{i \u003d 1, ..., n} \u003d N(s\u0027)$\n   - Compute the output predicted by the network for $s$: $N(s) \u003d \\{Q(s, a_i)\\}_{i \u003d 1, ..., n}$ \n   - Replace $Q(s, a_j)$ with $t$ to get $N\u0027(s)$\n   - Train the network using $s$ as the input and $N\u0027(s)$ as the output.  \n\nWe then use $s\u0027$ as the current state $s$ and reiterate.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "source": "class DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size \u003d state_size\n        self.action_size \u003d action_size\n        self.memory \u003d deque(maxlen\u003d2000)\n        self.gamma \u003d 0.95    # discount rate\n        self.epsilon \u003d 1.0  # exploration rate\n        self.epsilon_min \u003d 0.01\n        self.epsilon_decay \u003d 0.995\n        self.learning_rate \u003d 0.001\n        self.model \u003d self._build_model()\n\n    def _build_model(self):\n        # Neural Net for Deep-Q learning Model\n        model \u003d Sequential()\n        model.add(Dense(24, input_dim\u003dself.state_size, activation\u003d\u0027relu\u0027))\n        model.add(Dense(24, activation\u003d\u0027relu\u0027))\n        model.add(Dense(self.action_size, activation\u003d\u0027linear\u0027))\n        model.compile(loss\u003d\u0027mse\u0027, optimizer\u003dAdam(lr\u003dself.learning_rate))\n        return model\n\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state):\n        if np.random.rand() \u003c\u003d self.epsilon:\n            return random.randrange(self.action_size)\n        act_values \u003d self.model.predict(state)\n        return np.argmax(act_values[0])  # returns action\n\n    def replay(self, batch_size):\n        minibatch \u003d random.sample(self.memory, batch_size)\n        for state, action, reward, next_state, done in minibatch:\n            target \u003d reward\n            if not done:\n                target \u003d (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n            target_f \u003d self.model.predict(state)\n            target_f[0][action] \u003d target\n            self.model.fit(state, target_f, epochs\u003d1, verbose\u003d0)\n        if self.epsilon \u003e self.epsilon_min:\n            self.epsilon *\u003d self.epsilon_decay",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Gym environments\n\n### Frozen Lake - 16 states\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": "import gym\nimport numpy as np\nimport random\n\nenv \u003d gym.make(\u0027FrozenLake-v0\u0027)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n#### Description\nStates: There are 16 different states that represent the different parts of the lake. An agent wants to go from a starting point (S) to a goal (G) situated on the other side of the lake.\nSome states are traversable (F) but others are holes (H) that lead the agent to fall into the water.\nThe surface can be represented using the following grid:  \n  SFFF  \n  FHFH  \n  FFFH  \n  HFFG\n\nActions: There are 4 different actions that the agent can do when he is in a state: [LEFT, DOWN, RIGHT, UP].\n\nTransition model: If the agent wishes to execute an action, this action is executed correctly with a probability of 0.8 and causes the agent to move at a right angle with a probability of 0.2.\nThe 0.2 is distributed uniformly over the two possible right angles.\n\nRewards: +1 if the agent reaches the goal, -1 if the agent falls down a hole and -0.04 if the agent is on a frozen surface.\n\n#### Value Iteration",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[ 0.10923213 -0.0224833   0.15217449 -0.0224833   0.18551446  0.\n  0.258482    0.          0.39985603  0.68188442  0.64537105  0.\n  0.          0.81919453  0.94288425  0.        ]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "env \u003d env.unwrapped\ngamma \u003d 0.95\nmax_iter \u003d 100000\nepsilon \u003d 1e-20\nprint(value_iteration(env, gamma, max_iter, epsilon))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "To visualize the policy produced by this utility:",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "DRDL\nDHDH\nRDDH\nHRLH\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "def visualize_policy(policy):\n    visu \u003d \u0027\u0027\n    for k in range(len(policy)):\n        if k \u003e 0 and k%4 \u003d\u003d 0:\n            visu +\u003d \u0027\\n\u0027\n        if k \u003d\u003d 5 or k \u003d\u003d 7 or k \u003d\u003d 11 or k \u003d\u003d 12 or k \u003d\u003d 15:\n            visu+\u003d\u0027H\u0027\n        elif int(policy[k]) \u003d\u003d 0:\n            visu +\u003d \u0027L\u0027\n        elif int(policy[k]) \u003d\u003d 1:\n            visu +\u003d \u0027D\u0027\n        elif int(policy[k]) \u003d\u003d 2:\n            visu +\u003d \u0027R\u0027\n        elif int(policy[k]) \u003d\u003d 3:\n            visu +\u003d \u0027U\u0027\n    print(visu)\n    \nU \u003d value_iteration(env, gamma, max_iter, epsilon)\npolicy \u003d improve_policy(U, gamma)\nvisualize_policy(policy)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### Policy Iteration",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "DDDD\nDHDH\nRDDH\nHRDH\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "policy \u003d policy_iteration(env, gamma, max_iter, epsilon)\nvisualize_policy(policy)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### Q-Learning",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[[-0.2703593   0.01366858 -0.26953727 -0.25210457]\n [-0.33379303 -0.49290313 -0.34383339 -0.30714565]\n [-0.14452463 -0.1090627  -0.14078897 -0.14456021]\n [-0.15791265 -0.22621906 -0.20820038 -0.15800191]\n [-0.20335352  0.13353655 -0.51240193 -0.31623327]\n [ 0.          0.          0.          0.        ]\n [-0.18549375  0.04395889 -0.16658194 -0.15130954]\n [ 0.          0.          0.          0.        ]\n [-0.15296038 -0.26618784  0.25534461 -0.13390038]\n [-0.07284454  0.60071384 -0.05127881 -0.142625  ]\n [-0.00417099  0.56622879 -0.1444212  -0.00827035]\n [ 0.          0.          0.          0.        ]\n [ 0.          0.          0.          0.        ]\n [-0.22621906  0.14415937  0.7886408  -0.06465765]\n [ 0.09066991  0.12842186  0.95207074  0.07823609]\n [ 0.          0.          0.          0.        ]]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "alpha, gamma \u003d 0.05, 0.95\nnb_episodes, nb_steps \u003d 300, 100\nepsilon \u003d 0.1\nprint(q_learning(env, alpha, gamma, nb_episodes, nb_steps, epsilon))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "If we look at the action which maximizes the Q-value for each state we can visualize the policy produced by the Q-table:  ",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "DUDL\nDHDH\nRRDH\nHRRH\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "q_table \u003d q_learning(env, alpha, gamma, nb_episodes, nb_steps, epsilon)\ndef q_to_policy(Q):\n    policy \u003d []\n    for l in Q:\n        if l[0] \u003d\u003d l[1] \u003d\u003d l[2] \u003d\u003d l[3] \u003d\u003d 0.0:\n            policy.append(0)\n        else:\n            for k in range(0, len(l)):\n                if l[k] \u003d\u003d max(l):\n                    policy.append(k)\n                    break\n    return policy\n                    \nq_table \u003d q_learning(env, alpha, gamma, nb_episodes, nb_steps, epsilon)\npolicy \u003d q_to_policy(q_table)\nvisualize_policy(policy)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n#### Deep Q-Learning\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "source": "from collections import deque\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\nstate_size \u003d 1\naction_size \u003d env.action_space.n\nagent \u003d DQNAgent(state_size, action_size)\ndone \u003d False\nbatch_size \u003d 32\n\nfor e in range(nb_episodes):\n    state \u003d env.reset()\n    state \u003d np.reshape(state, [1, state_size])\n    for time in range(nb_steps):\n        action \u003d agent.act(state)\n        next_state, reward, done, _ \u003d env.step(action)\n        next_state \u003d np.reshape(next_state, [1, state_size])\n        agent.remember(state, action, reward, next_state, done)\n        state \u003d next_state\n        if done:\n            break\n        if len(agent.memory) \u003e batch_size:\n            agent.replay(batch_size)\n                \ntrained_network \u003d agent.model",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "If we use the trained network to predict the Q-values of all the possible actions for each state we get the following Q-table:",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[[-0.76135409 -0.8108964  -0.78342205 -0.78358257]\n [-0.77555388 -0.88860106 -0.78634799 -0.78616321]\n [-0.77420568 -0.94245815 -0.78825724 -0.78376877]\n [-0.78086573 -0.92055047 -0.79246944 -0.7923671 ]\n [-0.79521191 -0.83643091 -0.79979205 -0.81153494]\n [-0.80802393 -0.77505672 -0.80358922 -0.80420381]\n [-0.80460203 -0.75839829 -0.79603648 -0.79628098]\n [-0.81798756 -0.77963543 -0.8234151  -0.81082869]\n [-0.8292352  -0.79884332 -0.84636456 -0.82366037]\n [-0.78410727 -0.76391685 -0.78513217 -0.79044443]\n [-0.72800511 -0.71922374 -0.71389955 -0.7483899 ]\n [-0.56857026 -0.60667545 -0.51452321 -0.64284033]\n [-0.19239044 -0.37354037 -0.06866091 -0.41754788]\n [ 0.18375027 -0.14052671  0.37733489 -0.19189364]\n [ 0.55989069  0.09248677  0.82333034  0.03376043]\n [ 0.9203189   0.32429221  1.25621033  0.24979198]]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "def compute_q_table(env, network):\n    q_table \u003d np.zeros([env.observation_space.n, env.action_space.n])\n    for s in range(env.observation_space.n):\n        state \u003d np.zeros(1, int)\n        state[0] \u003d s\n        state \u003d np.reshape(state, [1, state_size])\n        q_table[s] \u003d list(network.predict(state)[0])\n    return q_table\n\nprint(compute_q_table(env, trained_network))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "The policy produced by this Q-table is the following one:",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "LLLL\nLHDH\nDDRH\nHRRH\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "q_table \u003d compute_q_table(env, trained_network)\npolicy \u003d q_to_policy(q_table)\nvisualize_policy(policy)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}