{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Markov Decision Process\n",
    "The state transition and reward models $R$ and $T$ are known.\n",
    "\n",
    "### Value Iteration\n",
    "The value function $U(s)$ represents the long-term reward that the agent is going to get if he starts in $s$ and follows the optimal policy.\n",
    "The value iteration approach keeps improving the value function at each iteration until it converges.\n",
    "At each iteration and for each state s we update its estimated utility:\n",
    "$$U_{t+1}(s) = \\max_{a}\\sum_{s'}T(s, a, s')(R(s') + \\gamma U_t(s')) $$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def value_iteration(env, max_iter, epsilon):\n",
    "    U = np.zeros(env.nS)\n",
    "    for i in range(max_iter):\n",
    "        prev_U = np.copy(U)\n",
    "        for s in range(env.nS):\n",
    "            list_sum = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                for p, s_prime, r, _ in env.P[s][a]:\n",
    "                    list_sum[a] += p*(r + prev_U[s_prime])\n",
    "            U[s] = max(list_sum)\n",
    "        if (np.sum(np.fabs(prev_U - U)) <= epsilon):\n",
    "            break\n",
    "    return U     "
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we have computed the true utility of each state $U(s)$ we can figure out the optimal policy $\\pi(s) = \\underset{a}{\\operatorname{argmax}}U(s) =  \\underset{a}{\\operatorname{argmax}}\\sum_{s'}T(s, a, s')(R(s')+\\gamma U(s'))$\n",
    "### Policy Iteration\n",
    "If we compute the true utility of each state $U(s)$ we can figure out the optimal policy but we have much more information than what we need to figure out the optimal policy.  \n",
    "The policy iteration approach re-defines the policy at each step and computes the value function associated to the current policy until the policy converges to the optimal policy.\n",
    "It needs less iterations than VI to converge however each iteration is more computationally expensive.  \n",
    "Given a policy $\\pi_t$ we compute the utility of each state:  \n",
    "  $$U_t(s) = \\sum_{s'}T(s, \\pi_t(s), s')(R(s') + \\gamma U_t(s')) $$"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, gamma, epsilon):\n",
    "    U = np.zeros(env.nS)\n",
    "    while True:\n",
    "        prev_U = np.copy(U)\n",
    "        for s in range(env.nS):\n",
    "            a = policy[s]\n",
    "            U[s] = sum([p * (r + gamma * prev_U[s_]) for p, s_, r, _ in env.P[s][a]])\n",
    "            #for p, s_prime, r, _ in env.P[s][a]:\n",
    "                #U[s] += p*(r + gamma*prev_U[s_prime])\n",
    "        if (np.sum(np.fabs(prev_U - U)) <= epsilon):\n",
    "            break\n",
    "    return U"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then improve the policy:  \n",
    "$$ \\pi_{t+1}(s) = \\underset{a}{\\operatorname{argmax}}\\sum_{s'}T(s, a, s')(R(s') + \\gamma U_t(s'))$$"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def improve_policy(U, gamma):\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        list_sum = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for p, s_prime, r, _ in env.P[s][a]:\n",
    "                list_sum[a] += p*(r+gamma*U[s_prime])\n",
    "        policy[s] = np.argmax(list_sum)\n",
    "    return policy"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "To get the final Policy Iteration algorithm we combine the two previous steps:"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma, max_iter, epsilon):\n",
    "    policy = np.random.choice(env.nA, env.nS)\n",
    "    for i in range(max_iter):\n",
    "        U = evaluate_policy(env, policy, gamma, epsilon)\n",
    "        new_policy = improve_policy(U, gamma)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Reinforcement Learning - Value Optimization\n",
    "The state transition and reward models $T$ and $R$ are not known. The agent has access to the set of possible states and actions and has to learn through interactions with the environment.\n",
    "\n",
    "### Q-Learning\n",
    "\n",
    "The Q-Learning algorithm does no longer have access to the models of the MDP that is to say the transition and reward functions.\n",
    "The idea is now to evaluate the Bellman equation from data by using transitions (data : $ <s, a, r, s'>$) to produce the solutions to the Q equations.\n",
    "At each episode we are going to update the estimates of the Q function coming from the previous episode through a learning rate $\\alpha$. \n",
    "$$ Q(s, a) = \\alpha(r + \\gamma \\max_{a'}Q(s', a')) + (1 - \\alpha)Q(s, a)$$\n"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def q_learning(env, alpha, gamma, nb_episodes, nb_steps, epsilon, epsilon_min, epsilon_decay):\n",
    "    \n",
    "    # Initialize the Q-table with zeros\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    \n",
    "    for i in range(nb_episodes):\n",
    "        s = env.reset() #Initial observation\n",
    "        for j in range(nb_steps):\n",
    "            # The action associated to s is the one that provides the best Q-value with a proba 1-epsilon and is random with a proba epsilon\n",
    "            if random.random() < 1 - epsilon:\n",
    "                a = np.argmax(Q[s,:]) \n",
    "            else : \n",
    "                a = np.random.randint(env.action_space.n)\n",
    "            # We get our transition <s, a, r, s'>\n",
    "            s_prime, r, d, _ = env.step(a)\n",
    "            # We update the Q-tqble with using new knowledge\n",
    "            Q[s, a] = alpha*(r + gamma*np.max(Q[s_prime,:])) + (1 - alpha)*Q[s, a]\n",
    "            s = s_prime\n",
    "            if d == True:\n",
    "                break\n",
    "            if (epsilon > epsilon_min):\n",
    "                epsilon *= epsilon_decay\n",
    "    \n",
    "    return Q\n"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deep Q-Learning\n",
    "\n",
    "The problem of the previous Q-Learning algorithm is that it will not be able to work in big state space environments.\n",
    "So rather than using a Q-table which returns a Q-value for a given state and a given action we can implement a neural network $N$ which takes a state and returns the Q-values of all the possible actions that could be taken in that state: $N(s) = \\{Q(s, a_1), Q(s, a_2), ..., Q(s, a_n)\\}$.   \n",
    "  Just as Q-Learning we start with an initial state $s$ and action $a$. We look at the next state $s'$ and the associated reward $r$ that the agent receives when he takes this action $a$ in the state $s$.\n",
    "  The transition $<s, a, r, s'>$ is stored in the memory of the agent. When we start to have enough transitions in the memory we sample a batch of them and for each transition $<s, a_j, r, s'>$ we do the following:  \n",
    "   - Compute a target $t$, which represents the \"best\" action that can be done when the agent is in $s$, that is to say the action that maximizes the expected long-term reward.  \n",
    "   $t = r +\\gamma \\max_a Q(s',a)$ with $\\{Q(s', a_i)\\}_{i = 1, ..., n} = N(s')$\n",
    "   - Compute the output predicted by the network for $s$: $N(s) = \\{Q(s, a_i)\\}_{i = 1, ..., n}$ \n",
    "   - Replace $Q(s, a_j)$ with $t$ to get $N'(s)$\n",
    "   - Train the network using $s$ as the input and $N'(s)$ as the output.  \n",
    "\n",
    "We then use $s'$ as the current state $s$ and reiterate."
   ],
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Target network\n",
    "Two networks are created. The first one is used to get the Q-values while the second one includes all updates in the training.\n",
    "The parameters of the target network $\\theta^-$ are copied every $\\tau$ updates from the online network and kept fixed on all other steps.\n",
    "Thanks to this target network the Q-value targets are fixed temporarily so that we do not have a moving target to chase. For each transition $<s, a_j, r, s'>$ we do the following:\n",
    "- Use the target network to compute a target $t = r +\\gamma \\max_a Q(s',a; \\theta^-)$\n",
    "- Use the online network to predict the output for $s$: $\\{Q(s, a_1;\\theta), ..., Q(s, a_n;\\theta)\\}$\n",
    "- Replace $Q(s, a_j;\\theta)$ with $t$ to get the corrected output for $s$\n",
    "- Train the online network using $s$ as the input and the corrected output as the output\n",
    "\n",
    "### Double Deep Q-Learning\n",
    "The goal of Double Q-Learning is to reduce overestimations made on action values by DQL by decomposing the max operation present in the target $t$ into action selection and action evaluation.  \n",
    "Indeed the target computed using DQL can be written as $t_{DQL} = r+\\gamma\\max_aQ(s',a;\\theta)$ where $\\theta$ represent the parameters of the network.  \n",
    "In the DDQL algorithm two sets of weights $\\theta$ and $\\theta'$ are used, one to determine the greedy policy and the other one to determine its value. \n",
    "The Double Q-learning target can thus be written $t_{DDQL} = r + \\gamma Q(s', \\underset{a}{\\operatorname{argmax}}Q(s',a;\\theta) ;\\theta')$. \n",
    "So we use one network to see which action $a$ maximizes the Q value associated to $s'$ and another network to evaluate the value of that action when it is associated to $s'$. For instance we could used the online network as $\\theta$ and the target network $\\theta^-$ as $\\theta'$.\n",
    "\n",
    "### Dueling Network Architecture\n",
    "Notations: The advantage function $A^{\\pi}(s,a) = Q^{\\pi}(s,a) - U^{\\pi}(s)$ subtracts the value that we get when we are in $s$ and follow $\\pi$ from the value that we get when we are in $s$, do $a$ and then follow $\\pi$. The advantage function thus measures the importance of each action.  \n",
    "Instead of using conventional architectures for DQL the idea is to use a dueling network representing two separate estimators. \n",
    "The state value function $U(s)$ is estimated separately from the state-dependent action advantage function $A(s,a)$.\n",
    "The idea is to allow the architecture to learn which states are valuable without having to learn the effect of each action for each state.  \n",
    "The two streams of the dueling Q-network are then combined via an aggregating layer to produce an estimate of $Q(s,a)$.  \n",
    "\n",
    "For many states it is unnecessary to estimate the value of each action choice however it is often important to estimate state values for every state. The state values are approximated in a better way thanks to this dueling architecture. In the classic architecture only the value for one of the action is updated and the values for the other actions remained untouched. Here the state values are updated with each update of the Q values.  \n",
    "The aggregating module is constructed as follows: $Q(s,a; \\theta, \\alpha, \\beta) = V(s; \\theta, \\beta) + (A(s,a; \\theta, \\alpha) - \\frac{1}{|A|}\\sum_{a'}A(s, a';\\theta, \\alpha))$ \n",
    "with $\\theta$ denoting the parameters of the convolutional layers and $\\alpha$ and $\\beta$ representing the parameters of the two streams of fully-connected layers.\n",
    "\n",
    "### Prioritized experience replay\n",
    "With $DQL$ we sample uniformly from the replay buffer while we would like to sample more frequently transitions from which there is much to learn.\n",
    "The priority of a transition $<s,a,r,s'>$ can be expressed as $p_i = |r + \\gamma \\max_{a'}Q(s,a';\\theta) - Q(s, a;\\theta^-)|$. The probability for this transition to be chosen for replay is thus $p = \\frac{p_i^{\\omega}}{\\sum_kp_k^{\\omega}}$ with $\\omega$ a hyper-parameter used to reintroduce randomness in the selection of the transitions. If $\\omega=0$ the transitions are selected randomly but if $\\omega=1$ only the transitions with the highest priorities are selected.  \n",
    "We also need to reduce the weights of the examples which are often seen to be sure that we do not update the weights only with a small portion of transitions that we believe are important.\n",
    "In order to do that we use importance sampling weights: $(\\frac{1}{N}.\\frac{1}{p})^b$. At the beginning of the learning $b=0$ so that these weights do not affect the sampling but at the end of the learning when the Q values begin to converge $b$ is set closer to $1$ so that we do not keep sampling the same group of transitions to train the network.\n",
    "### Distributional Q-Learning\n",
    "Remember, $Q^{\\pi}(s,a)$ represents the expected value of the total future rewards that we get if we are in $s$, do $a$ then follow the policy $\\pi$.  \n",
    "It would be more interesting to model the distribution of the total future rewards rather than restrict ourselves to the expected value which is the Q function.\n",
    "Indeed if the environment is stochastic and the distribution of future rewards follows a multimodal distribution then choosing actions based on expected value may lead to suboptimal outcomes.  \n",
    "A random variable $Z(s,a)$ called the Value Distribution is thus used instead of $Q(s,a)$ in the Bellman equation. $Z(s,a)$ which is no longer a scalar quantity like $Q(s,a)$ can be represented as a discrete distribution parameterized by a number of discrete values.  \n",
    "When does the distributional part come into play and make the network smarter about selecting the actions ?\n",
    "\n",
    "### Parameter Space Noise for Exploration\n",
    "Noise is injected directly in the agent's parameters instead of in the action space.  \n",
    "Indeed a single change to the weight vector can lead to a consistent state-dependent change in policy over multiple time-steps unlike state-independent exploration approaches such as $\\epsilon$\n",
    "-greedy where noise is added to the policy at every step. \n",
    "\n",
    "\n",
    "### Rainbow\n",
    "This algorithm combines six extensions of Deep Q-Learning which enhance its speed or stability:\n",
    "- Prioritized Experience Replay\n",
    "- Double DQN\n",
    "- Dueling Network Architecture\n",
    "- Noisy Nets for Exploration \n",
    "This extension thus combines the best of evolution strategies which use parameter perturbations but require lots of examples and traditional RL methods.\n",
    "- N-step Q-Learning: the rewards coming from $N$ previous steps are used instead of only the current reward when computing the target value.\n",
    "- Distributional Reinforcement Learning\n",
    "\n",
    "## Reinforcement Learning - Policy Optimization\n",
    "The idea is to consider parametric policies $\\pi_{\\theta}$ so that when we are in a state $s$ we can choose an action $a$ according to a parameter vector $\\theta$ of $n$ parameters. The goal is thus to tune this vector of parameters $\\theta$ in order to select the best action to take for policy $\\pi$.\n",
    "Policy-based methods allow us to directly learn the policy function mapping states to actions without having to learn a value function telling us what is the expected sum of rewards given a state and an action.  \n",
    "Advantages:  \n",
    "- Better convergence properties\n",
    "- More effective in high-dimensional or continuous action spaces since the goal is to learn a set of parameters and not to assign a score for each possible action given the current state.\n",
    "- Able to learn stochastic policies while value functions can not.  \n",
    "\n",
    "Disadvantages:\n",
    "- Often converge on a local maximum rather than on the global optimum\n",
    "\n",
    "### Policy Gradient\n",
    "\n",
    "The goal of policy gradient algorithms is to optimize parameterized policies with respect to the long-term cumulative reward. $\\pi_{\\theta}(a|s)$ outputs the probability of taking action $a$ given state $s$ with parameters $\\theta$. We have to find the best parameters $\\theta$ to maximize a score function $J(\\theta)$.  \n",
    "The first step is thus to measure the quality of a policy $\\pi$ with a score function $J(\\theta)$ and the second step is to use policy gradient ascent to find the best parameter $\\theta$ that improves $\\pi$.  \n",
    "The score function can also be written: $J(\\theta) = E_{\\pi}(Q^{\\pi}(s,a)) = \\sum_sd^{\\pi}(s)\\sum_a \\pi_{\\theta}(a|s)Q^{\\pi}(s,a)$ where $d^{\\pi}(s)$ represents the state distribution under $\\pi$.  \n",
    "We can show that $\\nabla_{\\theta}J(\\theta) \\propto E_{\\pi}(Q^{\\pi}(s,a)\\nabla_{\\theta}\\ln \\pi_{\\theta}(a|s))$  \n",
    "The parameters of the policy are then updated the following way: $\\theta_{k+1} = \\theta_k + \\alpha_k\\nabla_{\\theta}J(\\theta_{k})$  \n",
    "Policy Gradient algorithms cause the parameters to move most in the direction that favors actions with the highest returns.\n",
    "\n",
    "#### Reinforce (Monte-Carlo policy gradient)\n",
    "This method uses real sample trajectories to update the policy parameter $\\theta$.\n",
    "Algorithm:\n",
    "- Initialize the policy parameter $\\theta$ at random\n",
    "- Generate one trajectory on policy $\\pi_{\\theta}$: $S_1, A_1, R_2, S_2, A_2, ..., S_T$\n",
    "- For $t=1, 2, ..., T$:\n",
    "    - Estimate $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2R_{t+3} + ...$ \n",
    "    - Update the parameter $\\theta = \\theta + \\alpha G_t \\nabla_{\\theta}\\ln \\pi_{\\theta}(A_t|S_t)$\n",
    "\n",
    "#### Actor Critic \n",
    "This method possesses two models:\n",
    "- Critic updates the value function parameters $\\omega$\n",
    "- Actor updates the policy parameters $\\theta$ in the direction suggested by the value function  \n",
    "\n",
    "Algorithm:\n",
    "- Initialize $s, \\theta, \\omega$ at random, sample $a$ according to $\\pi_{\\theta}(a|s)$\n",
    "- For $t=1, ..., T$:\n",
    "    - Sample reward $r$ and next state $s'$\n",
    "    - Sample the next action $a'$ according to $\\pi_{\\theta}(a'|s')$\n",
    "    - Update the policy parameters $\\theta = \\theta + \\alpha_{\\theta}Q_{\\omega}(s,a)\\nabla_{\\theta}\\ln\\pi_{\\theta}(a|s)$\n",
    "    - Compute the correction for the Q-function: $\\delta = r + \\gamma Q_{\\omega}(s',a') - Q_{\\omega}(s,a)$  \n",
    "    Update the parameters of the value function: $\\omega = \\omega + \\alpha_{\\omega}\\delta_t\\nabla_{\\omega}Q_{\\omega}(s,a)$\n",
    "    - $a = a'$ and $s = s'$\n",
    "\n",
    "#### Off-Policy Policy Gradient\n",
    "Unlike the two previous methods this method does not collect training samples according to the policy that we try to optimize for. Instead it uses a behavioral policy $\\beta(a|s)$ to collect training observations. \n",
    "The score of the policy that we try to optimize is thus defined as $J(\\theta) = E_{s\\beta}(Q^{\\pi}(s,a)) = \\sum_sd^{\\beta}(s)\\sum_a \\pi_{\\theta}(a|s)Q^{\\pi}(s,a)$  \n",
    "The gradient can be rewritten as $\\nabla_{\\theta}J(\\theta) = E_{\\beta}(\\frac{\\pi_{\\theta}(a|s)}{\\beta(a|s)}Q^{\\pi}(s,a)\\nabla_{\\theta}\\ln \\pi_{\\theta}(a|s))$\n",
    "     \n",
    "#### Asynchronous Advantage Actor-Critic\n",
    "This method focuses on parallel training.  \n",
    "We start by constructing a global network with two output layers, one for the value function and one for the policy. Then a set of agents possessing their own network and environment are created. Each of these workers run on a separate processor thread.  \n",
    "To start with each worker sets its network parameters to the ones of the global network. Then the workers interact with their copy of the environment and collect experiences $(s, a, r, s', \\mbox{done}, \\mbox{value})$ to compute value and policy losses which are then used to get gradients.\n",
    "Each worker uses these gradients to update the global network's parameters.  \n",
    "Once an update is made the worker resets its network parameters to the ones of the global network and the process repeats.\n",
    "\n",
    "#### Deterministic Policy Gradient\n",
    "This method models the policy as a deterministic decision: $\\pi(s) =\\mu(s) = a$\n",
    "#### Deep Deterministic Policy Gradient\n",
    "This off-policy actor critic algorithm combines Deterministic Policy Gradient with Deep Q-Network.\n",
    "#### Proximal Policy Optimization\n",
    "The idea is to avoid parameter updates that change the policy too much at one step.\n",
    "#### Actor Critic with experience replay\n",
    "This method is A3C's off-policy counterpart.\n",
    "#### Soft Actor Critic\n",
    "This is an off-policy actor critic model following the maximum entropy reinforcement learning framework. \n"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "stem_cell": {
   "cell_type": "raw",
   "source": "...#%% md\n## Markov Decision Process \nThe state transition and reward models $R$ and $T$ are known.\n\n### Value Iteration\nThe value function $V(s)$ represents the long-term reward that the agent is going to get if he starts in $s$ and follows the optimal policy.  \n\nThe value iteration approach keeps improving the value function at each iteration until it converges.\nAt each iteration and for each state s we update its estimated utility:  \n  $$V_{t+1}(s) = \\max_{a}\\sum_{s'}T(s, a, s')(R(s') + \\gamma V_t(s')) $$  \n",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}