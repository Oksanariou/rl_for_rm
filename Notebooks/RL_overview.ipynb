{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Markov Decision Process \nThe state transition and reward models $R$ and $T$ are known.\n\n### Value Iteration\nThe value function $U(s)$ represents the long-term reward that the agent is going to get if he starts in $s$ and follows the optimal policy.  \n\nThe value iteration approach keeps improving the value function at each iteration until it converges.\nAt each iteration and for each state s we update its estimated utility:  \n  $$U_{t+1}(s) \u003d \\max_{a}\\sum_{s\u0027}T(s, a, s\u0027)(R(s\u0027) + \\gamma U_t(s\u0027)) $$  \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def value_iteration(env, max_iter, epsilon):\n    U \u003d np.zeros(env.nS)\n    for i in range(max_iter):\n        prev_U \u003d np.copy(U)\n        for s in range(env.nS):\n            list_sum \u003d np.zeros(env.nA)\n            for a in range(env.nA):\n                for p, s_prime, r, _ in env.P[s][a]:\n                    list_sum[a] +\u003d p*(r + prev_U[s_prime])\n            U[s] \u003d max(list_sum)\n        if (np.sum(np.fabs(prev_U - U)) \u003c\u003d epsilon):\n            break\n    return U     ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Once we have computed the true utility of each state $U(s)$ we can figure out the optimal policy $\\pi(s) \u003d \\underset{a}{\\operatorname{argmax}}U(s) \u003d  \\underset{a}{\\operatorname{argmax}}\\sum_{s\u0027}T(s, a, s\u0027)(R(s\u0027)+\\gamma U(s\u0027))$\n### Policy Iteration\nIf we compute the true utility of each state $U(s)$ we can figure out the optimal policy but we have much more information than what we need to figure out the optimal policy.  \nThe policy iteration approach re-defines the policy at each step and computes the value function associated to the current policy until the policy converges to the optimal policy.\nIt needs less iterations than VI to converge however each iteration is more computationally expensive.  \nGiven a policy $\\pi_t$ we compute the utility of each state:  \n  $$U_t(s) \u003d \\sum_{s\u0027}T(s, \\pi_t(s), s\u0027)(R(s\u0027) + \\gamma U_t(s\u0027)) $$",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def evaluate_policy(env, policy, gamma, epsilon):\n    U \u003d np.zeros(env.nS)\n    while True:\n        prev_U \u003d np.copy(U)\n        for s in range(env.nS):\n            a \u003d policy[s]\n            U[s] \u003d sum([p * (r + gamma * prev_U[s_]) for p, s_, r, _ in env.P[s][a]])\n            #for p, s_prime, r, _ in env.P[s][a]:\n                #U[s] +\u003d p*(r + gamma*prev_U[s_prime])\n        if (np.sum(np.fabs(prev_U - U)) \u003c\u003d epsilon):\n            break\n    return U",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "We then improve the policy:  \n$$ \\pi_{t+1}(s) \u003d \\underset{a}{\\operatorname{argmax}}\\sum_{s\u0027}T(s, a, s\u0027)(R(s\u0027) + \\gamma U_t(s\u0027))$$",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def improve_policy(U, gamma):\n    policy \u003d np.zeros(env.nS)\n    for s in range(env.nS):\n        list_sum \u003d np.zeros(env.nA)\n        for a in range(env.nA):\n            for p, s_prime, r, _ in env.P[s][a]:\n                list_sum[a] +\u003d p*(r+gamma*U[s_prime])\n        policy[s] \u003d np.argmax(list_sum)\n    return policy",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\nTo get the final Policy Iteration algorithm we combine the two previous steps:",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def policy_iteration(env, gamma, max_iter, epsilon):\n    policy \u003d np.random.choice(env.nA, env.nS)\n    for i in range(max_iter):\n        U \u003d evaluate_policy(env, policy, gamma, epsilon)\n        new_policy \u003d improve_policy(U, gamma)\n        if (np.all(policy \u003d\u003d new_policy)):\n            break\n        policy \u003d new_policy\n    return policy",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n## Reinforcement Learning - Value Optimization\nThe state transition and reward models $T$ and $R$ are not known. The agent has access to the set of possible states and actions and has to learn through interactions with the environment.\n\n### Q-Learning\n\nThe Q-Learning algorithm does no longer have access to the models of the MDP that is to say the transition and reward functions.\nThe idea is now to evaluate the Bellman equation from data by using transitions (data : $ \u003cs, a, r, s\u0027\u003e$) to produce the solutions to the Q equations.\nAt each episode we are going to update the estimates of the Q function coming from the previous episode through a learning rate $\\alpha$. \n$$ Q(s, a) \u003d \\alpha(r + \\gamma \\max_{a\u0027}Q(s\u0027, a\u0027)) + (1 - \\alpha)Q(s, a)$$\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def q_learning(env, alpha, gamma, nb_episodes, nb_steps, epsilon, epsilon_min, epsilon_decay):\n    \n    # Initialize the Q-table with zeros\n    Q \u003d np.zeros([env.observation_space.n, env.action_space.n])\n    \n    for i in range(nb_episodes):\n        s \u003d env.reset() #Initial observation\n        for j in range(nb_steps):\n            # The action associated to s is the one that provides the best Q-value with a proba 1-epsilon and is random with a proba epsilon\n            if random.random() \u003c 1 - epsilon:\n                a \u003d np.argmax(Q[s,:]) \n            else : \n                a \u003d np.random.randint(env.action_space.n)\n            # We get our transition \u003cs, a, r, s\u0027\u003e\n            s_prime, r, d, _ \u003d env.step(a)\n            # We update the Q-tqble with using new knowledge\n            Q[s, a] \u003d alpha*(r + gamma*np.max(Q[s_prime,:])) + (1 - alpha)*Q[s, a]\n            s \u003d s_prime\n            if d \u003d\u003d True:\n                break\n            if (epsilon \u003e epsilon_min):\n                epsilon *\u003d epsilon_decay\n    \n    return Q\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Deep Q-Learning\n\nThe problem of the previous Q-Learning algorithm is that it will not be able to work in big state space environments.\nSo rather than using a Q-table which returns a Q-value for a given state and a given action we can implement a neural network $N$ which takes a state and returns the Q-values of all the possible actions that could be taken in that state: $N(s) \u003d \\{Q(s, a_1), Q(s, a_2), ..., Q(s, a_n)\\}$.   \n  Just as Q-Learning we start with an initial state $s$ and action $a$. We look at the next state $s\u0027$ and the associated reward $r$ that the agent receives when he takes this action $a$ in the state $s$.\n  The transition $\u003cs, a, r, s\u0027\u003e$ is stored in the memory of the agent. When we start to have enough transitions in the memory we sample a batch of them and for each transition $\u003cs, a_j, r, s\u0027\u003e$ we do the following:  \n   - Compute a target $t$, which represents the \"best\" action that can be done when the agent is in $s$, that is to say the action that maximizes the expected long-term reward.  \n   $t \u003d r +\\gamma \\max_a Q(s\u0027,a)$ with $\\{Q(s\u0027, a_i)\\}_{i \u003d 1, ..., n} \u003d N(s\u0027)$\n   - Compute the output predicted by the network for $s$: $N(s) \u003d \\{Q(s, a_i)\\}_{i \u003d 1, ..., n}$ \n   - Replace $Q(s, a_j)$ with $t$ to get $N\u0027(s)$\n   - Train the network using $s$ as the input and $N\u0027(s)$ as the output.  \n\nWe then use $s\u0027$ as the current state $s$ and reiterate.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "class DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size \u003d state_size\n        self.action_size \u003d action_size\n        self.memory \u003d deque(maxlen\u003d2000)\n        self.gamma \u003d 0.95    # discount rate\n        self.epsilon \u003d 1.0  # exploration rate\n        self.epsilon_min \u003d 0.01\n        self.epsilon_decay \u003d 0.995\n        self.learning_rate \u003d 0.001\n        self.model \u003d self._build_model()\n\n    def _build_model(self):\n        # Neural Net for Deep-Q learning Model\n        model \u003d Sequential()\n        model.add(Dense(24, input_dim\u003dself.state_size, activation\u003d\u0027relu\u0027))\n        model.add(Dense(24, activation\u003d\u0027relu\u0027))\n        model.add(Dense(self.action_size, activation\u003d\u0027linear\u0027))\n        model.compile(loss\u003d\u0027mse\u0027, optimizer\u003dAdam(lr\u003dself.learning_rate))\n        return model\n\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state):\n        if np.random.rand() \u003c\u003d self.epsilon:\n            return random.randrange(self.action_size)\n        act_values \u003d self.model.predict(state)\n        return np.argmax(act_values[0])  # returns action\n\n    def replay(self, batch_size):\n        minibatch \u003d random.sample(self.memory, batch_size)\n        for state, action, reward, next_state, done in minibatch:\n            target \u003d reward\n            if not done:\n                target \u003d (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n            target_f \u003d self.model.predict(state)\n            target_f[0][action] \u003d target\n            self.model.fit(state, target_f, epochs\u003d1, verbose\u003d0)\n        if self.epsilon \u003e self.epsilon_min:\n            self.epsilon *\u003d self.epsilon_decay",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### Target network\nTwo networks are created. The first one is used to get the Q-values while the second one includes all updates in the training.\nThe parameters of the target network $\\theta^-$ are copied every $\\tau$ updates from the online network and kept fixed on all other steps.\nThanks to this target network the Q-value targets are fixed temporarily so that we do not have a moving target to chase. For each transition $\u003cs, a_j, r, s\u0027\u003e$ we do the following:\n- Use the target network to compute a target $t \u003d r +\\gamma \\max_a Q(s\u0027,a; \\theta^-)$\n- Use the online network to predict the output for $s$: $\\{Q(s, a_1;\\theta), ..., Q(s, a_n;\\theta)\\}$\n- Replace $Q(s, a_j;\\theta)$ with $t$ to get the corrected output for $s$\n- Train the online network using $s$ as the input and the corrected output as the output\n\n### Double Deep Q-Learning\nThe goal of Double Q-Learning is to reduce overestimations made on action values by DQL by decomposing the max operation present in the target $t$ into action selection and action evaluation.  \nIndeed the target computed using DQL can be written as $t_{DQL} \u003d r+\\gamma\\max_aQ(s\u0027,a;\\theta)$ where $\\theta$ represent the parameters of the network.  \nIn the DDQL algorithm two sets of weights $\\theta$ and $\\theta\u0027$ are used, one to determine the greedy policy and the other one to determine its value. \nThe Double Q-learning target can thus be written $t_{DDQL} \u003d r + \\gamma Q(s\u0027, \\underset{a}{\\operatorname{argmax}}Q(s\u0027,a;\\theta) ;\\theta\u0027)$. \nSo we use one network to see which action $a$ maximizes the Q value associated to $s\u0027$ and another network to evaluate the value of that action when it is associated to $s\u0027$. For instance we could used the online network as $\\theta$ and the target network $\\theta^-$ as $\\theta\u0027$.\n\n### Dueling Network Architectures\nNotations: The advantage function $A^{\\pi}(s,a) \u003d Q^{\\pi}(s,a) - U^{\\pi}(s)$ subtracts the value that we get when we are in $s$ and follow $\\pi$ from the value that we get when we are in $s$, do $a$ and then follow $\\pi$. The advantage function thus measures the importance of each action.  \nInstead of using conventional architectures for DQL the idea is to use a dueling network representing two separate estimators. \nThe state value function $U(s)$ is estimated separately from the state-dependent action advantage function $A(s,a)$.\nThe idea is to allow the architecture to learn which states are valuable without having to learn the effect of each action for each state.  \nThe two streams of the dueling Q-network are then combined via an aggregating layer to produce an estimate of $Q(s,a)$.  \n\nFor many states it is unnecessary to estimate the value of each action choice however it is often important to estimate state values for every state. The state values are approximated in a better way thanks to this dueling architecture. In the classic architecture only the value for one of the action is updated and the values for the other actions remained untouched. Here the state values are updated with each update of the Q values.\n\n### Prioritized experience replay\nWith $DQL$ we sample uniformly from the replay buffer while we would like to sample more frequently transitions from which there is much to learn.\nThe priority of a transition $\u003cs,a,r,s\u0027\u003e$ can be expressed as $p_i \u003d |r + \\gamma \\max_{a\u0027}Q(s,a\u0027;\\theta) - Q(s, a;\\theta^-)|$. The probability for this transition to be chosen for replay is thus $p \u003d \\frac{p_i^{\\omega}}{\\sum_kp_k^{\\omega}}$ with $\\omega$ a hyper-parameter used to reintroduce randomness in the selection of the transitions. If $\\omega\u003d0$ the transitions are selected randomly but if $\\omega\u003d1$ only the transitions with the highest priorities are selected.  \nWe also need to reduce the weights of the examples which are often seen to be sure that we do not update the weights only with a small portion of transitions that we believe are important.\nIn order to do that we use importance sampling weights: $(\\frac{1}{N}.\\frac{1}{p})^b$. At the beginning of the learning $b\u003d0$ so that these weights do not affect the sampling but at the end of the learning when the Q values begin to converge $b$ is set closer to $1$ so that we do not keep sampling the same group of transitions to train the network.\n### Distributional Q-Learning\nRemember, $Q^{\\pi}(s,a)$ represents the expected value of the total future rewards that we get if we are in $s$, do $a$ then follow the policy $\\pi$.  \nIt would be more interesting to model the distribution of the total future rewards rather than restrict ourselves to the expected value which is the Q function.\nIndeed if the environment is stochastic and the distribution of future rewards follows a multimodal distribution then choosing actions based on expected value may lead to suboptimal outcomes.  \nA random variable $Z(s,a)$ called the Value Distribution is thus used instead of $Q(s,a)$ in the Bellman equation. $Z(s,a)$ which is no longer a scalar quantity like $Q(s,a)$ can be represented as a discrete distribution parameterized by a number of discrete values.  \nWhen does the distributional part come into play and make the network smarter about selecting the actions ?\n\n## Reinforcement Learning - Policy Optimization\nThe idea is to consider parametric policies $\\pi_{theta}$ so that when we are in a state $s$ we can choose an action $a$ according to a parameter vector $\\theta$ of $n$ parameters. Te goal is thus to tune this vector of parameters $\\theta$ in order to select the best action to take for policy $\\pi$.\nPolicy-based methods allow us to directly learn the policy function mapping states to actions without having to learn a value function telling us what is the expected sum of rewards given a state and an action.  \nAdvantages:  \n- Better convergence properties\n- More effective in high-dimensional or continuous action spaces since the goal is to learn a set of parameters and not to assign a score for each possible action given the current state.\n- Able to learn stochastic policies while value functions can not.  \n\nDisadvantages:\n- Often converge on a local maximum rather than on the global optimum\n\n### Policy Gradient\n\nThe goal of policy gradient algorithms is to optimize parameterized policies with respect to the long-term cumulative reward. $\\pi_{\\theta}(a|s)$ outputs the probability of taking action $a$ given state $s$ with parameters $\\theta$. We have to find the best parameters $\\theta$ to maximize a score function $J(\\theta)$.  \nThe first step is thus to measure the quality of a policy $\\pi$ with a score function $J(\\theta)$ and the second step is to use policy gradient ascent to find the best parameter $\\theta$ that improves $\\pi$.  \nThe score function can also be written: $J(\\theta) \u003d E_{\\pi}(Q^{\\pi}(s,a)) \u003d \\sum_sd^{\\pi}(s)\\sum_a \\pi_{\\theta}(a|s)Q^{\\pi}(s,a)$ where $d^{\\pi}(s)$ represents the state distribution under $\\pi$.  \nWe can show that $\\nabla_{\\theta}J(\\theta) \\propto E_{\\pi}(Q^{\\pi}(s,a)\\nabla_{\\theta}\\ln \\pi_{\\theta}(a|s))$  \nThe parameters of the policy are then updated the following way: $\\theta_{k+1} \u003d \\theta_k + \\alpha_k\\nabla_{\\theta}J(\\theta_{k})$  \nPolicy Gradient algorithms cause the parameters to move most in the direction that favors actions with the highest returns.\n\n#### Reinforce (Monte-Carlo policy gradient)\nThis method uses real sample trajectories to update the policy parameter $\\theta$.\nAlgorithm:\n- Initialize the policy parameter $\\theta$ at random\n- Generate one trajectory on policy $\\pi_{\\theta}$: $S_1, A_1, R_2, S_2, A_2, ..., S_T$\n- For $t\u003d1, 2, ..., T$:\n    - Estimate $G_t \u003d R_{t+1} + \\gamma R_{t+2} + \\gamma^2R_{t+3} + ...$ \n    - Update the parameter $\\theta \u003d \\theta + \\alpha G_t \\nabla_{\\theta}\\ln \\pi_{\\theta}(A_t|S_t)$\n\n#### Actor Critic \nThis method possesses two models:\n- Critic updates the value function parameters $\\omega$\n- Actor updates the policy parameters $\\theta in the direction suggested by the value function  \n\nAlgorithm:\n- Initialize $s, \\theta, \\omega$ at random, sample $a$ according to $\\pi_{\\theta}(a|s)$\n- For $t\u003d1, ..., T$:\n    - Sample reward $r$ and next state $s\u0027$\n    - Sample the next action $a\u0027$ according to $\\pi_{\\theta}(a\u0027|s\u0027)$\n    - Update the policy parameters $\\theta \u003d \\theta + \\alpha_{\\theta}Q_{\\omega}(s,a)\\nabla_{\\theta}\\ln\\pi_{\\theta}(a|s)$\n    - Compute the correction for the Q-function: $\\delta \u003d r + \\gamma Q_{\\omega}(s\u0027,a\u0027) - Q_{\\omega}(s,a)$  \n    Update the parameters of the value function: $\\omega \u003d \\omega + \\alpha_{\\omega}\\delta_t\\nabla_{\\omega}Q_{\\omega}(s,a)$\n    - $a \u003d a\u0027$ and $s \u003d s\u0027$\n\n#### Off-Policy Policy Gradient\nUnlike the two previous methods this method does not collect training samples according to the policy that we try to optimize for. Instead it uses a behavioral policy $\\beta(a|s)$ to collect training observations. \nThe score of the policy that we try to optimize is thus defined as $J(\\theta) \u003d E_{s\\beta}(Q^{\\pi}(s,a)) \u003d \\sum_sd^{\\beta}(s)\\sum_a \\pi_{\\theta}(a|s)Q^{\\pi}(s,a)$  \nThe gradient can be rewritten as $\\nabla_{\\theta}J(\\theta) \u003d E_{\\beta}(\\frac{\\pi_{\\theta}(a|s)}{\\beta(a|s)}Q^{\\pi}(s,a)\\nabla_{\\theta}\\ln \\pi_{\\theta}(a|s))$\n     \n#### Asynchronous Advantage Actor-Critic\nThis method focuses on parallel training.  \nWe start by constructing a global network with two output layers, one for the value function and one for the policy. Then a set of agents possessing their own network and environment are created. Each of these workers run on a separate processor thread.  \nTo start with each worker sets its network parameters to the ones of the global network. Then the workers interact with their copy of the environment and collect experiences $(s, a, r, s\u0027, \\mbox{done}, \\mbox{value})$ to compute value and policy losses which are then used to get gradients.\nEach worker uses these gradients to update the global network\u0027s parameters.  \nOnce an update is made the worker resets its network parameters to the ones of the global network and the process repeats.\n\n#### Deterministic Policy Gradient\nThis method models the policy as a deterministic decision: $\\pi(s) \u003d\\mu(s) \u003d a$\n#### Deep Deterministic Policy Gradient\nThis off-policy actor critic algorithm combines Deterministic Policy Gradient with Deep Q-Network.\n#### Proximal Policy Optimization\nThe idea is to avoid parameter updates that change the policy too much at one step.\n#### Actor Critic with experience replay\nThis method is A3C\u0027s off-policy counterpart.\n#### Soft Actor Critic\nThis is an off-policy actor critic model following the maximum entropy reinforcement learning framework. \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}